# Technical Specification

Source files for this section:
- `scripts/litkg_cli.py`
- `scripts/phase1_integration.py`

### Technical Summary

#### Business Logic Overview

The project consists of two main scripts, each encapsulating distinct business logic for the LitKG (Literature Knowledge Graph) system:

1. **litkg_cli.py**: This script is a CLI wrapper designed to ensure the `litkg` package is accessible and executable. It dynamically adjusts the Python path to include the project's source directory, facilitating the execution of the CLI.

2. **phase1_integration.py**: This script integrates the various components of Phase 1 of the LitKG pipeline, encompassing literature processing, knowledge graph (KG) preprocessing, entity linking, and dataset integration. It also includes validation, analysis, and preparation steps for Phase 2.

#### Unique Business Logic and Domain-Specific Implementations

##### Phase 1 Integration Pipeline (phase1_integration.py)

- **Literature Processing**: The script processes biomedical literature based on predefined queries. It utilizes a `LiteratureProcessor` to fetch and process documents, extracting entities and relations. The results are serialized and saved for further use.

- **Knowledge Graph Preprocessing**: The KG is downloaded, processed, and integrated. This involves downloading data, processing it to build a graph structure, and saving the integrated KG. Statistics about the KG (number of entities and relations) are logged.

- **Entity Linking**: The script performs entity linking between literature entities and KG entities using a custom `EntityLinker`. It utilizes semantic and contextual information to achieve high-quality matches. The linking results are serialized and saved.

- **Integrated Dataset Creation**: An integrated dataset is created by combining literature documents, KG entities and relations, and entity linking results. This dataset includes metadata, statistics, and serialized entities and relations. It is saved in JSON format for subsequent phases.

- **Validation and Analysis**: The integration results are validated and analyzed to assess the quality of the Phase 1 output. This includes calculating statistics such as the number of documents, entities, relations, and successful links. Quality assessments are performed to evaluate high-confidence linking rates and entity type coverage.

- **Preparation for Phase 2**: The script prepares data structures for Phase 2, which involves training a hybrid Graph Neural Network (GNN). Nodes and edges are created based on literature and KG entities and relations, along with their features. This structured data is saved in JSON format for use in GNN training.

#### File Paths
- **phase1_integration.py**: Contains the core business logic for the Phase 1 integration pipeline, including literature processing, KG preprocessing, entity linking, dataset integration, validation, analysis, and preparation for Phase 2.

Source files for this section:
- `scripts/example_conversational_agents.py`

### Technical Summary of Business Logic in `scripts/example_conversational_agents.py`

#### Overview
The script showcases a comprehensive conversational AI system tailored for biomedical research, emphasizing unique business logic in several key areas:

1. **Biomedical Knowledge Retrieval with RAG Agent**:
   - **Domain-Specific Knowledge Retrieval**: The script demonstrates a specialized biomedical RAG (Retrieval-Augmented Generation) agent capable of retrieving context-aware knowledge from biomedical literature. 
   - **Entity Tracking and Context Management**: The RAG agent maintains context across conversations, tracking biomedical entities and relationships.

2. **Multi-Agent Orchestration**:
   - **Intelligent Task Delegation**: The system employs an agent orchestrator to intelligently route tasks to specialized agents based on context and confidence scoring.
   - **Performance Tracking**: It tracks agent performance and confidence levels to ensure optimal task delegation.

3. **Research Workflow Automation**:
   - **Automated Multi-Step Research Workflows**: The script demonstrates automated workflows such as "hypothesis_to_experiment," guiding researchers through multi-step research processes.
   - **Workflow Execution and Monitoring**: It monitors workflow progress and status, ensuring seamless execution of complex research tasks.

4. **Interactive Research Conversation**:
   - **Natural Language Interface**: The system provides a conversational interface that retains context across turns, enabling natural language interactions for biomedical research.
   - **Intelligent Follow-Up Suggestions**: It generates relevant follow-up questions to guide researchers in their exploration.

5. **Report Generation**:
   - **Comprehensive Demo Report**: The script generates a detailed report summarizing the demonstration results, highlighting key metrics and research impact.
   - **Human-Readable Summary**: It creates a summary text file providing an overview of the demo's capabilities and performance highlights.

### Key Business Logic Components
- **Biomedical RAG Agent**: 
  - Utilizes vector similarity search for knowledge retrieval.
  - Maintains context and tracks biomedical entities.
- **Agent Orchestration**: 
  - Routes tasks based on agent performance and confidence.
  - Tracks and logs agent interactions.
- **Research Workflows**: 
  - Automates multi-step research processes.
  - Monitors and logs workflow execution.
- **Interactive Conversation**: 
  - Retains context across conversation turns.
  - Generates intelligent follow-up suggestions.
- **Report Generation**: 
  - Creates detailed and summary reports of demo results.

### Relevant File Path
- `scripts/example_conversational_agents.py`

Source files for this section:
- `scripts/example_novel_discovery_system.py`

### Technical Summary of Business Logic in `scripts/example_novel_discovery_system.py`

#### Unique Business Logic and Domain-Specific Implementations

1. **Novel Relation Prediction**:
   - Utilizes a hybrid approach combining Graph Neural Networks (GNN) and pattern analysis for predicting novel biomedical relationships.
   - Implements biological plausibility validation using Language Learning Models (LLM).
   - Thresholds for confidence (0.6) and novelty (0.7) are applied to filter predictions.
   - File: `scripts/example_novel_discovery_system.py`

2. **Hypothesis Generation**:
   - Generates testable biomedical hypotheses from predicted novel relations.
   - Uses LLM for hypothesis generation and multi-agent systems for validation.
   - Prioritizes hypotheses based on scores for confidence, novelty, and feasibility.
   - Provides experimental approaches and key predictions for each hypothesis.
   - File: `scripts/example_novel_discovery_system.py`

3. **Comprehensive Validation System**:
   - Incorporates literature cross-validation, temporal validation, and expert validation.
   - Generates expert assessment forms for further validation.
   - Summarizes validation results including precision, recall, F1-Score, and AUC.
   - File: `scripts/example_novel_discovery_system.py`

4. **Discovery Report Generation**:
   - Generates multiple reports including novel relations discovery, hypothesis generation, comprehensive validation, and an executive summary.
   - Reports are categorized and include high-level summaries, top discoveries, and validation results.
   - Human-readable summary report provides an overview of the discovery process, key findings, and next steps.
   - File: `scripts/example_novel_discovery_system.py`

#### Key Business Logic Highlights

- **Hybrid GNN and Pattern Analysis**: Combines neural network predictions with pattern-based discovery from literature.
- **LLM-Powered Validation**: Uses advanced language models to validate biological plausibility and generate hypotheses.
- **Multi-Agent Hypothesis Validation**: Employs multiple AI agents to validate generated hypotheses.
- **Comprehensive Validation Pipeline**: Integrates multiple validation methods including literature, temporal, and expert validation.
- **Prioritized Research Reports**: Generates detailed reports with prioritized recommendations and experimental approaches.

This summary captures the unique business logic and domain-specific implementations within the novel discovery system, focusing on the innovative approaches and workflows designed for biomedical knowledge discovery.

Source files for this section:
- `scripts/example_phase3_confidence_scoring.py`

### Technical Summary of Phase 3 Confidence Scoring Logic

**File Path:** `scripts/example_phase3_confidence_scoring.py`

**Overview:**
This script implements a comprehensive confidence scoring system for evaluating the reliability of biomedical relationships derived from both literature and experimental evidence. The system is designed to support Phase 3 novel knowledge discovery by distinguishing reliable predictions from uncertain ones.

**Unique Business Logic and Domain-Specific Implementations:**

1. **Synthetic Evidence Generation:**
   - Generates synthetic biomedical relationship data representing various evidence scenarios (e.g., strong literature + strong experimental evidence, conflicting evidence, literature-only, experimental-only).
   - Each scenario is designed to demonstrate different aspects of confidence scoring.

2. **Confidence Scoring:**
   - **Literature Confidence Assessment:**
     - Evaluates literature evidence based on journal quality, citations, recency, methodology, consensus, and statistical rigor.
   - **Experimental Confidence Assessment:**
     - Evaluates experimental evidence based on sample size, statistical significance, replication, consistency, and data quality.
   - **Cross-Modal Evidence Integration:**
     - Integrates literature and experimental evidence to produce an overall confidence score.
     - Resolves conflicts between literature and experimental evidence.
   - **Uncertainty Quantification:**
     - Differentiates between epistemic uncertainty (model-related) and aleatoric uncertainty (data-related).

3. **Batch Processing:**
   - Demonstrates batch assessment of multiple relationships to improve efficiency and scalability.

4. **Visualization and Reporting:**
   - Generates comprehensive visualizations to illustrate confidence scores, literature vs. experimental confidence, cross-modal agreement, and uncertainty quantification.
   - Exports confidence results to JSON for further analysis and generates a summary report highlighting key findings.

**Domain-Specific Requirements and Implementations:**

- **Evidence Types:** The system handles different types of evidence (literature, experimental) and their unique characteristics.
- **Conflict Resolution:** Implements logic to resolve conflicts between literature and experimental evidence, providing a unified confidence score.
- **Uncertainty Types:** Differentiates between epistemic and aleatoric uncertainty, providing a more nuanced understanding of confidence levels.
- **Batch Processing:** Designed to handle large-scale assessments efficiently, crucial for real-world biomedical data analysis.

This script exemplifies the critical business logic required for reliable biomedical relationship evaluation, essential for advancing novel knowledge discovery in the field.

Source files for this section:
- `scripts/example_langchain_integration.py`
- `scripts/example_literature_processing.py`

### Technical Summary of Unique Business Logic

#### LangChain Integration Demo (scripts/example_langchain_integration.py)

**Enhanced Document Processing:**
- Demonstrates intelligent document loading and biomedical-aware text chunking.
- Uses LangChain's `RecursiveCharacterTextSplitter` for advanced document segmentation.

**LLM-Powered Entity Extraction:**
- Utilizes large language models (LLMs) for context-aware entity recognition and relation extraction.
- Implements confidence scoring with reasoning, using multiple model consensus for reliability.

**Vector Similarity Search:**
- Introduces vector-based semantic search across biomedical literature.
- Demonstrates the use of embeddings and vector stores (e.g., FAISS) for contextual retrieval.

**Comparison with Phase 1 Pipeline:**
- Highlights key improvements over the traditional Phase 1 approach:
  - Higher accuracy in entity extraction (90%+ vs. 70-80%)
  - Better handling of complex biomedical terminology
  - Contextual understanding of relationships
  - Confidence scoring with explanations
  - Ability to extract implicit relationships

**Exporting Demo Results:**
- Exports results and generates a summary report detailing the benefits and capabilities demonstrated by the LangChain integration.

#### Literature Processing Pipeline (scripts/example_literature_processing.py)

**Literature Processor Initialization:**
- Initializes a `LiteratureProcessor` to search and process biomedical literature.

**Processing Specific Queries:**
- Demonstrates processing of specific biomedical literature queries:
  - Cancer genomics literature (e.g., "BRCA1 breast cancer mutation treatment")
  - Drug-disease literature (e.g., "immunotherapy checkpoint inhibitor melanoma")

**Entity and Relation Extraction:**
- Extracts entities and relations from processed documents, providing statistical summaries such as entity type distribution and relation type distribution.

**Loading and Analyzing Saved Results:**
- Loads previously saved results for further analysis, focusing on high-confidence entities (>0.8 confidence).
- Provides insights into the most frequent high-confidence entities in the processed literature.

### Unique Business Logic Highlights

- **Biomedical-aware Text Processing:** 
  - Both scripts implement specialized text processing tailored for biomedical literature, including intelligent chunking and LLM-enhanced extraction.

- **Semantic Search Implementation:**
  - Introduction of vector-based semantic search in the LangChain integration demo, enabling advanced retrieval capabilities.

- **Enhanced Entity and Relation Extraction:**
  - Utilization of LLMs for more accurate and contextually aware entity and relation extraction, with confidence scoring.

- **Comparison with Traditional Approaches:**
  - Detailed comparison highlighting the significant improvements offered by the LangChain integration over the Phase 1 pipeline.

- **Statistical Analysis of Extracted Data:**
  - Provision of statistical summaries and insights into the extracted entities and relations, aiding in further analysis and understanding of the processed literature.

Source files for this section:
- `scripts/example_entity_linking.py`
- `scripts/example_kg_preprocessing.py`

### Business Logic Summary

#### Entity Linking Demonstration (`scripts/example_entity_linking.py`)

1. **Entity Linking Process:**
   - Utilizes fuzzy matching and disambiguation to link entities from literature to a knowledge graph (KG).
   - Supports semantic and contextual matching strategies.
   - Analyzes linking performance by calculating match rates, confidence scores, and similarity scores.
   - Demonstrates integration of literature and KG data, showcasing cross-modal evidence and potential novel associations.

2. **Linking Strategies Comparison:**
   - Compares different linking strategies (fuzzy only, fuzzy + semantic, fuzzy + context, full pipeline) to evaluate their effectiveness.

3. **Data Transformation:**
   - Creates dummy literature data for demonstration purposes if real data is unavailable.
   - Generates example entities and relations to simulate processed documents.

#### Knowledge Graph Preprocessing Demonstration (`scripts/example_kg_preprocessing.py`)

1. **Data Integration:**
   - Downloads and processes data from multiple sources (CIVIC, TCGA, CPTAC).
   - Standardizes and integrates entities into a unified knowledge graph.

2. **Graph Analysis:**
   - Analyzes the integrated knowledge graph to provide statistics on entities, relations, and data sources.
   - Identifies top connected genes and diseases, and analyzes cross-source relations.

3. **Ontology Mapping:**
   - Demonstrates mapping of entities to Unified Medical Language System (UMLS) and Gene Ontology (GO) identifiers.

### Relevant File Paths
- `scripts/example_entity_linking.py`
- `scripts/example_kg_preprocessing.py`

Source files for this section:
- `scripts/example_ollama_integration.py`

## Technical Summary of Business Logic in `scripts/example_ollama_integration.py`

### Business Logic and Domain-Specific Implementations

#### 1. **Ollama Setup and Model Management**
   - **Model Recommendations:** The script provides a mechanism to recommend models tailored for biomedical tasks, considering factors like task type and memory constraints (`demonstrate_ollama_setup`).
   - **Biomedical Model Setup:** It automates the installation of biomedical models, ensuring that relevant models are available for local inference (`demonstrate_ollama_setup`).

#### 2. **Local LLM Inference with Biomedical Examples**
   - **Biomedical Query Processing:** The script demonstrates processing of specific biomedical queries using locally installed models, showcasing the capability to handle domain-specific questions (`demonstrate_local_inference`).
   - **Performance Measurement:** It includes logic to measure and log performance metrics (response time, tokens per second, memory usage) for local LLM inference, providing insights into the efficiency of local models (`demonstrate_local_inference`).

#### 3. **Specialized Biomedical Chains**
   - **Task-Specific Chains:** The script implements specialized chains for various biomedical tasks such as entity extraction, relation extraction, hypothesis generation, and validation. Each chain is tailored to handle specific types of biomedical data and queries (`demonstrate_biomedical_chains`).

#### 4. **Unified LLM Interface Across Providers**
   - **Multi-Provider Interface:** It demonstrates a unified interface that abstracts the complexity of interacting with multiple LLM providers, allowing for seamless switching between local and cloud-based models (`demonstrate_unified_interface`).
   - **Model Recommendations for Tasks:** The interface provides recommendations for the best models to use for specific biomedical tasks, considering constraints like cost and local availability (`demonstrate_unified_interface`).

#### 5. **Local Model Management**
   - **Model Installation and Benchmarking:** The script includes logic for installing biomedical models and benchmarking their performance on specific prompts, ensuring optimal model selection for given tasks (`demonstrate_model_management`).

#### 6. **Comprehensive Report Generation**
   - **Ollama Integration Report:** It generates a detailed report summarizing the setup, performance, and capabilities demonstrated during the Ollama integration process, providing a comprehensive overview of the local LLM inference capabilities (`generate_ollama_report`).

### Unique Business Logic Highlights
- **Domain-Specific Model Recommendations:** Tailored for biomedical tasks, considering memory constraints and task-specific requirements.
- **Performance Benchmarking:** Custom logic to measure and log performance metrics specific to biomedical LLM inference.
- **Specialized Biomedical Chains:** Unique implementations for handling various biomedical tasks, such as entity and relation extraction, hypothesis generation, and validation.
- **Unified LLM Interface:** Abstraction layer for interacting with multiple LLM providers, with model recommendations tailored for biomedical tasks.
- **Comprehensive Reporting:** Generation of detailed reports summarizing the Ollama integration process, highlighting local inference capabilities and performance metrics.

### Relevant File Path
- `scripts/example_ollama_integration.py`

Source files for this section:
- `scripts/example_phase2_hybrid_gnn.py`

### Technical Summary of Business Logic in `scripts/example_phase2_hybrid_gnn.py`

#### Unique Business Logic and Domain-Specific Implementations

1. **Graph Construction from Literature and Knowledge Graph (KG) Data**:
   - **File Path**: `scripts/example_phase2_hybrid_gnn.py`
   - **Business Logic**:
     - Constructs literature graphs from biomedical documents and KG subgraphs from biomedical entities and relations.
     - Aligns entities between literature and KG modalities to create training graph pairs.
     - Utilizes biomedical embeddings to represent entities and relations.

2. **Hybrid Graph Neural Network (GNN) Model with Cross-Modal Attention**:
   - **File Path**: `scripts/example_phase2_hybrid_gnn.py`
   - **Business Logic**:
     - Implements a hybrid GNN architecture that integrates literature and KG graphs.
     - Employs cross-modal attention mechanisms to enhance node representations by attending to relevant information across modalities.
     - Supports multi-task learning for both link prediction and relation prediction.

3. **Multi-Task Learning for Link and Relation Prediction**:
   - **File Path**: `scripts/example_phase2_hybrid_gnn.py`
   - **Business Logic**:
     - Trains the hybrid GNN model to predict links between entities and classify relation types.
     - Utilizes a combination of loss functions (link loss, relation loss, confidence loss, contrastive loss) to optimize model performance.

4. **Attention Pattern Visualization**:
   - **File Path**: `scripts/example_phase2_hybrid_gnn.py`
   - **Business Logic**:
     - Visualizes attention patterns generated by the cross-modal attention mechanism.
     - Provides insights into how the model attends to different entities across literature and KG graphs, aiding in interpretability and debugging.

5. **Complete Workflow Demonstration**:
   - **File Path**: `scripts/example_phase2_hybrid_gnn.py`
   - **Business Logic**:
     - Demonstrates the end-to-end workflow of graph construction, model setup, training, and attention visualization.
     - Highlights the integration of literature and KG data for enhanced knowledge discovery in biomedical research.

### Key Points
- The script focuses on constructing and training a hybrid GNN model specifically tailored for biomedical literature and knowledge graphs.
- It emphasizes cross-modal attention mechanisms to improve model performance in predicting links and relations.
- The demonstration includes visualization of attention patterns to provide interpretability of the model's decisions.
- The workflow is designed to facilitate knowledge discovery in the biomedical domain by integrating heterogeneous data sources.

Source files for this section:
- `scripts/example_ml_integration.py`

### Technical Summary of Business Logic in `scripts/example_ml_integration.py`

#### Unique Business Logic and Domain-Specific Implementations

1. **HuggingFace Biomedical Model Integration:**
   - **Model Demonstration:** Showcases the use of HuggingFace's biomedical transformer models for various NLP tasks.
   - **Available Models Display:** Lists available biomedical models with detailed information including name, description, tasks, domain, size, and citation.
   - **Embedding Generation:** Demonstrates generating embeddings using PubMedBERT for biomedical texts.
   - **Named Entity Recognition (NER):** Extracts entities from biomedical texts using BioBERT.
   - **Text Classification:** Classifies biomedical texts into predefined categories using PubMedBERT.
   - **Question Answering:** Answers biomedical questions using PubMedBERT with provided context.

2. **Biomedical Embeddings:**
   - **Entity Embeddings:** Generates embeddings for biomedical entities, considering context.
   - **Similarity Search:** Finds similar entities based on embeddings with a specified threshold.
   - **Pairwise Similarity:** Computes pairwise similarity between entities using their embeddings.
   - **Clustering:** Clusters biomedical entities based on their embeddings into a specified number of clusters.

3. **PyTorch Graph Models:**
   - **Cross-Modal Attention:** Implements a custom CrossModalAttention model to align literature and knowledge graph (KG) embeddings.
   - **Hybrid Graph Neural Network (GNN):** Introduces a HybridGNN model that integrates literature and KG data for graph-based learning.
   - **Link Prediction:** Utilizes the HybridGNN model to predict links between literature and KG entities.

4. **Integration Workflow:**
   - **Biomedical Text Processing:** Processes biomedical literature to extract entities and generate embeddings.
   - **Knowledge Graph Simulation:** Simulates the integration of a knowledge graph with literature data.
   - **Entity Linking:** Links entities from biomedical literature to knowledge graph entities based on embedding similarity.
   - **Phase 2 Preparation:** Outlines the next steps for training a hybrid GNN on literature-KG pairs and evaluating link prediction performance.

#### Relevant File Path
- `scripts/example_ml_integration.py`

Source files for this section:
- `README.md`

### LitKG-Integrate: Unique Business Logic and Domain-Specific Implementations

#### 1. **LLM-Powered Entity and Relation Extraction**
- **Business Logic**: Utilizes GPT-4/Claude for sophisticated entity and relation extraction, achieving over 90% accuracy compared to traditional methods (70-80%). 
- **Domain-Specific Implementation**: 
  - **Few-shot Prompting**: Enhances LLM extraction with minimal examples.
  - **Chain-of-Thought Reasoning**: Contextualizes relation extraction for better accuracy.
- **File Path**: `src/litkg/langchain_integration/llm_entity_extractor.py`

#### 2. **Vector Similarity Search**
- **Business Logic**: Implements semantic search across biomedical literature using domain-specific embeddings.
- **Domain-Specific Implementation**: 
  - **Biomedical Embeddings**: Utilizes embeddings tailored for biomedical text.
  - **Efficient Storage**: Uses vector databases like Chroma/FAISS for fast semantic search.
- **File Path**: `src/litkg/langchain_integration/vector_search.py`

#### 3. **Conversational Agents for Biomedical Queries**
- **Business Logic**: Provides a natural language interface for biomedical queries and hypothesis generation.
- **Domain-Specific Implementation**: 
  - **Persistent Memory**: Maintains context across user interactions.
  - **Biomedical Knowledge Integration**: Combines literature and experimental data for informed responses.
- **File Path**: `src/litkg/langchain_integration/conversational_agent.py`

#### 4. **Retrieval-Augmented Generation (RAG) Systems**
- **Business Logic**: Enhances literature analysis with context-aware responses using RAG.
- **Domain-Specific Implementation**: 
  - **Literature-Augmented Responses**: Integrates real-time literature retrieval with GNN predictions.
- **File Path**: `src/litkg/langchain_integration/rag_system.py`

#### 5. **Multi-Modal Integration**
- **Business Logic**: Seamlessly combines textual and experimental evidence for comprehensive analysis.
- **Domain-Specific Implementation**: 
  - **Cross-Modal Attention Mechanisms**: Enhanced with LangChain retrievers for optimal evidence weighting.
- **File Path**: `src/litkg/phase2/hybrid_gnn.py`

#### 6. **Enhanced Entity Linking**
- **Business Logic**: Improves disambiguation of entities using LLM consensus scoring.
- **Domain-Specific Implementation**: 
  - **Ontology Mapping**: Standardizes entities across datasets like CIVIC, TCGA, CPTAC.
- **File Path**: `src/litkg/phase1/entity_linking.py`

#### 7. **Confidence Scoring with LLM Explanations**
- **Business Logic**: Provides multi-modal assessment of predictions with LLM-powered explanations.
- **Domain-Specific Implementation**: 
  - **Explainable AI**: Generates reasoning for all predictions to enhance trust and understanding.
- **File Path**: `src/litkg/phase3/confidence_scoring.py`

#### 8. **Hypothesis Generation Agents**
- **Business Logic**: Employs multi-step reasoning agents for novel target discovery.
- **Domain-Specific Implementation**: 
  - **AI-Powered Discovery**: Automates the generation of novel hypotheses based on integrated data.
- **File Path**: `src/litkg/phase3/hypothesis_generation.py`

#### 9. **Temporal Dynamics in Literature Integration**
- **Business Logic**: Incorporates publication dates to reflect evolving understanding in biomedical research.
- **Domain-Specific Implementation**: 
  - **Dynamic Updates**: Integrates latest publications for real-time knowledge discovery.
- **File Path**: `src/litkg/phase1/literature_processing.py`

#### 10. **Uncertainty Quantification**
- **Business Logic**: Distinguishes between "unknown" and "contradictory" evidence to provide clearer insights.
- **Domain-Specific Implementation**: 
  - **Evidence Assessment**: Uses LLM to evaluate the nature of conflicting information.
- **File Path**: `src/litkg/phase3/uncertainty_quantification.py`

## System Overview
Processing error for 38 files in group 10.


Source files for this section:
- `Makefile`
- `env.template`

### Technical Summary of LitKG-Integrate Business Logic

#### Unique Business Logic and Domain-Specific Implementations

1. **Literature Processing Integration**:
   - **File**: `scripts/example_literature_processing.py`
   - **Business Logic**:
     - Fetches and processes scientific literature using PubMed API.
     - Extracts relevant data for integration into the knowledge graph.

2. **Knowledge Graph Preprocessing**:
   - **File**: `scripts/example_kg_preprocessing.py`
   - **Business Logic**:
     - Preprocesses data to create a structured knowledge graph.
     - Implements domain-specific ontology mappings and entity linking.

3. **Entity Linking**:
   - **File**: `scripts/example_entity_linking.py`
   - **Business Logic**:
     - Links entities within the literature to standardized biomedical ontologies.
     - Utilizes UMLS API for enhanced ontology mapping.

4. **Machine Learning Integration**:
   - **File**: `scripts/example_ml_integration.py`
   - **Business Logic**:
     - Integrates machine learning models (e.g., HuggingFace) for text analysis and entity recognition.
     - Implements custom pipelines for model training and inference.

5. **Hybrid GNN Architecture**:
   - **File**: `scripts/example_phase2_hybrid_gnn.py`
   - **Business Logic**:
     - Implements a hybrid Graph Neural Network (GNN) architecture for advanced knowledge graph analysis.
     - Combines graph-based and sequence-based models for improved performance.

6. **Confidence Scoring**:
   - **File**: `scripts/example_phase3_confidence_scoring.py`
   - **Business Logic**:
     - Assigns confidence scores to entities and relationships within the knowledge graph.
     - Utilizes domain-specific algorithms to evaluate the reliability of data integrations.

7. **LangChain Integration**:
   - **File**: `scripts/example_langchain_integration.py`
   - **Business Logic**:
     - Integrates LangChain for natural language processing tasks.
     - Implements custom workflows for text summarization and question answering.

8. **Novel Discovery System**:
   - **File**: `scripts/example_novel_discovery_system.py`
   - **Business Logic**:
     - Implements a system for discovering novel relationships and insights within the biomedical literature.
     - Utilizes advanced graph algorithms and machine learning models.

9. **Ollama Local LLM Integration**:
   - **File**: `scripts/example_ollama_integration.py`
   - **Business Logic**:
     - Integrates Ollama for local Large Language Model (LLM) deployment.
     - Implements custom inference pipelines for biomedical text analysis.

10. **Conversational Agents and RAG Systems**:
    - **File**: `scripts/example_conversational_agents.py`
    - **Business Logic**:
      - Deploys conversational agents for interactive querying of the knowledge graph.
      - Implements Retrieval-Augmented Generation (RAG) systems for enhanced response generation.

#### Domain-Specific Requirements and Implementations

- **PubMed API Integration**: Required for fetching scientific literature.
- **UMLS API Usage**: Optional but recommended for better ontology mapping.
- **Neo4j and MongoDB Integration**: Optional for advanced knowledge graph storage and querying.
- **Weights & Biases Integration**: Optional for experiment tracking and logging.

#### Custom Validation Rules and Business Policies

- **Environment Setup**: Ensures all required API keys and configurations are correctly set up before running any integration pipelines.
- **Logging and Caching**: Customizable logging levels and cache directories to manage resource usage and performance.

This summary captures the unique business logic and domain-specific implementations within the LitKG-Integrate project, focusing exclusively on the core functionalities and workflows that distinguish this project from generic implementations.

Source files for this section:
- `scripts/setup_models.py`

### Technical Summary of Business Logic in `scripts/setup_models.py`

#### Unique Business Logic and Domain-Specific Implementations

1. **Model Installation and Verification**:
   - **Scispacy Models**: The script automates the installation of specific biomedical models (`en_core_sci_sm`, `en_core_sci_md`, `en_ner_bc5cdr_md`, `en_ner_bionlp13cg_md`) required for the project. It also verifies the installations by loading a sample document and extracting entities, ensuring the models function as expected in a biomedical context.
   - **Transformers Models**: Verifies the installation of specialized biomedical transformers models like PubMedBERT and BioBERT by loading them and logging success messages.

2. **Directory Setup**:
   - Creates project-specific directories (`data/raw`, `data/processed`, `data/external`, `cache`, `logs`, `models`, `results`) tailored for handling biomedical data and model outputs.

3. **Environment Template Creation**:
   - Generates a `.env` template file with placeholders for essential API keys and database URLs specific to biomedical research, such as PubMed API, AI API keys (Anthropic Claude, OpenAI), and UMLS API.

4. **Basic Functionality Testing**:
   - Tests the integration of biomedical NLP components by loading configuration, initializing the `BiomedicalNLP` class, and performing entity extraction on a sample text. This ensures that the NLP pipeline functions correctly within the biomedical domain.

### Relevant File Path
- `scripts/setup_models.py`